{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masato.morita/Library/Caches/pypoetry/virtualenvs/recommender-experiments-_FVBVT8O-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Callable, Optional, TypedDict\n",
    "import numpy as np\n",
    "from obp.dataset import SyntheticBanditDataset\n",
    "from obp.ope import ReplayMethod, InverseProbabilityWeighting\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditFeedbackDict(TypedDict):\n",
    "    n_rounds: int  # ラウンド数\n",
    "    n_actions: int  # アクション数\n",
    "    context: np.ndarray  # 文脈 (shape: (n_rounds, dim_context))\n",
    "    action_context: (\n",
    "        np.ndarray\n",
    "    )  # アクション特徴量 (shape: (n_actions, dim_action_features))\n",
    "    action: np.ndarray  # 実際に選択されたアクション (shape: (n_rounds,))\n",
    "    position: Optional[np.ndarray]  # ポジション (shape: (n_rounds,) or None)\n",
    "    reward: np.ndarray  # 報酬 (shape: (n_rounds,))\n",
    "    expected_reward: np.ndarray  # 期待報酬 (shape: (n_rounds, n_actions))\n",
    "    pi_b: np.ndarray  # データ収集方策 P(a|x) (shape: (n_rounds, n_actions))\n",
    "    pscore: np.ndarray  # 傾向スコア (shape: (n_rounds,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4 0.3 0.2 0.2]\n",
      " [0.4 0.3 0.2 0.2]\n",
      " [0.4 0.3 0.2 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# 真の期待報酬関数 E_{p(r|x,a)}[r] を定義する\n",
    "def expected_reward_function(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"(アクションa, 文脈x)の各組み合わせに対する期待報酬 E_{p(r|x,a)}[r] を定義する関数\n",
    "    今回の場合は、推薦候補4つの記事を送った場合の報酬rの期待値を、文脈xに依存しない固定値として設定する\n",
    "    ニュース0: 0.2, ニュース1: 0.15, ニュース2: 0.1, ニュース3: 0.05\n",
    "    返り値のshape: (n_rounds, n_actions, len_list)\n",
    "    \"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 固定の期待報酬を設定 (n_actions=4として固定値を設定)\n",
    "    fixed_rewards = np.array([0.4, 0.3, 0.2, 0.2])\n",
    "\n",
    "    # 文脈の数だけ期待報酬を繰り返して返す\n",
    "    return np.tile(fixed_rewards, (n_rounds, 1))\n",
    "\n",
    "\n",
    "# 試しに期待報酬関数を実行してみる\n",
    "n_rounds = 3\n",
    "n_actions = 4\n",
    "context = np.array([[1], [2], [3]])\n",
    "action_context = np.array([[1], [2], [3], [4]])\n",
    "print(expected_reward_function(context, action_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.1 0.1 0.7]\n",
      " [0.1 0.1 0.1 0.7]\n",
      " [0.1 0.1 0.1 0.7]]\n"
     ]
    }
   ],
   "source": [
    "# 5種類の推薦方策を定義していく\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def pi_1(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_2(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_1$ を確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([0.0, 1.0, 0.0, 0.0])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_3(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率0.4、\n",
    "    ニュース $a_1$ を確率0.3、ニュース $a_2$ を確率0.2、ニュース $a_3$ を確率0.1で\n",
    "    推薦する確率的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_4(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"ユーザとニュースのコンテキストを考慮し、\n",
    "    コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを\n",
    "    確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 内積を計算\n",
    "    scores = context @ action_context.T  # shape: (n_rounds, n_actions)\n",
    "\n",
    "    # 各ラウンドで最もスコアが高いアクションのindexを取得\n",
    "    selected_actions = np.argmax(scores, axis=1)  # shape: (n_rounds,)\n",
    "\n",
    "    # 決定的方策: 確率1で最もスコアが高いアクションを選択\n",
    "    action_dist = np.zeros((n_rounds, n_actions))\n",
    "    action_dist[np.arange(n_rounds), selected_actions] = 1.0\n",
    "\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_5(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"ユーザとニュースのコンテキストを考慮し、\n",
    "    コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを\n",
    "    確率0.7で推薦し、その他のニュースを均等に確率0.1で推薦する確率的方策。\n",
    "    \"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 内積を計算\n",
    "    scores = context @ action_context.T  # shape: (n_rounds, n_actions)\n",
    "\n",
    "    # 各ラウンドで最もスコアが高いアクションのindexを取得\n",
    "    selected_actions = np.argmax(scores, axis=1)  # shape: (n_rounds,)\n",
    "\n",
    "    # 確率的方策: 確率0.4で全てのアクションを一様ランダムに選択し、確率0.6で最もスコアが高いアクションを決定的に選択\n",
    "    epsilon = 0.4\n",
    "    action_dist = np.full((n_rounds, n_actions), epsilon / n_actions)\n",
    "    action_dist[np.arange(n_rounds), selected_actions] = (\n",
    "        1.0 - epsilon + epsilon / n_actions\n",
    "    )\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "print(\n",
    "    pi_5(\n",
    "        context=np.array([[1], [2], [3]]),\n",
    "        action_context=np.array([[1], [2], [3], [4]]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]]\n",
      "(3, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "def policy_wrapper(\n",
    "    base_policy: Callable[[np.ndarray, np.ndarray, int], np.ndarray],\n",
    "    mode: Literal[\"logging_poicy\", \"target_policy\"],\n",
    ") -> Callable[[np.ndarray, np.ndarray, int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    方策のラッパー関数。データ収集時と評価時で出力の形状を切り替える。\n",
    "    (logging_policyの場合はshape=(n_rounds, n_actions)、target_policyの場合はshape=(n_rounds,n_actions, 1))\n",
    "    Parameters\n",
    "    - base_policy: ベースとなる方策関数\n",
    "    - mode: 方策のモード。\"logging_policy\"はデータ収集方策、\"target_policy\"は評価方策\n",
    "\n",
    "    Returns\n",
    "    - wrapされた方策関数\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapped_policy(\n",
    "        context: np.ndarray,\n",
    "        action_context: np.ndarray,\n",
    "        random_state: int = None,\n",
    "    ) -> np.ndarray:\n",
    "        action_dist = base_policy(context, action_context, random_state)\n",
    "        if mode == \"logging_policy\":\n",
    "            return action_dist\n",
    "        elif mode == \"target_policy\":\n",
    "            return action_dist[:, :, np.newaxis]\n",
    "\n",
    "    return wrapped_policy\n",
    "\n",
    "\n",
    "target_policy = policy_wrapper(pi_5, mode=\"target_policy\")\n",
    "print(target_policy(context, action_context))\n",
    "print(target_policy(context, action_context).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10_000, 6)\n",
      "┌───────────┬───────────────────────┬────────┬──────────────────────────────────┬────────┬─────────┐\n",
      "│ time_step ┆ context               ┆ action ┆ action_context                   ┆ reward ┆ p_score │\n",
      "│ ---       ┆ ---                   ┆ ---    ┆ ---                              ┆ ---    ┆ ---     │\n",
      "│ i64       ┆ list[f64]             ┆ i64    ┆ list[f64]                        ┆ i64    ┆ f64     │\n",
      "╞═══════════╪═══════════════════════╪════════╪══════════════════════════════════╪════════╪═════════╡\n",
      "│ 0         ┆ [1.764052, 0.400157,  ┆ 1      ┆ [0.59047, 0.110831, 0.67821]     ┆ 1      ┆ 0.3     │\n",
      "│           ┆ 0.978738]             ┆        ┆                                  ┆        ┆         │\n",
      "│ 1         ┆ [2.240893, 1.867558,  ┆ 2      ┆ [0.194671, 0.242354, 0.164083]   ┆ 1      ┆ 0.2     │\n",
      "│           ┆ -0.977278…            ┆        ┆                                  ┆        ┆         │\n",
      "│ 2         ┆ [0.950088, -0.151357, ┆ 1      ┆ [0.59047, 0.110831, 0.67821]     ┆ 0      ┆ 0.3     │\n",
      "│           ┆ -0.10321…             ┆        ┆                                  ┆        ┆         │\n",
      "│ 3         ┆ [0.410599, 0.144044,  ┆ 1      ┆ [0.59047, 0.110831, 0.67821]     ┆ 1      ┆ 0.3     │\n",
      "│           ┆ 1.454274]             ┆        ┆                                  ┆        ┆         │\n",
      "│ 4         ┆ [0.761038, 0.121675,  ┆ 1      ┆ [0.59047, 0.110831, 0.67821]     ┆ 0      ┆ 0.3     │\n",
      "│           ┆ 0.443863]             ┆        ┆                                  ┆        ┆         │\n",
      "│ …         ┆ …                     ┆ …      ┆ …                                ┆ …      ┆ …       │\n",
      "│ 9995      ┆ [-0.303895, 0.213129, ┆ 1      ┆ [0.59047, 0.110831, 0.67821]     ┆ 0      ┆ 0.3     │\n",
      "│           ┆ 0.046172…             ┆        ┆                                  ┆        ┆         │\n",
      "│ 9996      ┆ [1.533356, 1.910297,  ┆ 0      ┆ [0.897946, 0.146114, 0.001808]   ┆ 1      ┆ 0.4     │\n",
      "│           ┆ 0.069245]             ┆        ┆                                  ┆        ┆         │\n",
      "│ 9997      ┆ [1.039309, 0.118204,  ┆ 2      ┆ [0.194671, 0.242354, 0.164083]   ┆ 0      ┆ 0.2     │\n",
      "│           ┆ -0.762623…            ┆        ┆                                  ┆        ┆         │\n",
      "│ 9998      ┆ [0.606083, -0.400898, ┆ 0      ┆ [0.897946, 0.146114, 0.001808]   ┆ 1      ┆ 0.4     │\n",
      "│           ┆ -0.58545…             ┆        ┆                                  ┆        ┆         │\n",
      "│ 9999      ┆ [-1.511276, 0.976844, ┆ 2      ┆ [0.194671, 0.242354, 0.164083]   ┆ 0      ┆ 0.2     │\n",
      "│           ┆ 1.255501…             ┆        ┆                                  ┆        ┆         │\n",
      "└───────────┴───────────────────────┴────────┴──────────────────────────────────┴────────┴─────────┘\n",
      "{'simulate_idx': 0, 'ground_truth_policy_value': 0.2687, 'estimated_policy_value_by_naive': 0.3015075376884422, 'estimated_policy_value_by_ips': 0.25658333333333333}\n"
     ]
    }
   ],
   "source": [
    "def run_single_simulation(\n",
    "    simulate_idx: int,\n",
    "    n_rounds: int,\n",
    "    n_actions: int,\n",
    "    dim_context: int,\n",
    "    reward_type: str,\n",
    "    action_context: np.ndarray,\n",
    "    reward_function: Callable[[np.ndarray, np.ndarray, int], np.ndarray],\n",
    "    logging_policy_function: Callable[[np.ndarray, np.ndarray, int], np.ndarray],\n",
    "    target_policy_function: Callable[[np.ndarray, np.ndarray, int], np.ndarray],\n",
    ") -> dict:\n",
    "    # データ収集方策によって集められるはずの、擬似バンディットデータの設定を定義\n",
    "    dataset = SyntheticBanditDataset(\n",
    "        n_actions=n_actions,\n",
    "        dim_context=dim_context,\n",
    "        reward_type=reward_type,\n",
    "        reward_function=reward_function,\n",
    "        behavior_policy_function=logging_policy_function,\n",
    "        random_state=simulate_idx,\n",
    "        action_context=action_context,\n",
    "    )\n",
    "    # 収集されるバンディットフィードバックデータを生成\n",
    "    bandit_feedback = dataset.obtain_batch_bandit_feedback(n_rounds=n_rounds)\n",
    "\n",
    "    # bandit_feedbackをpl.DataFrameに変換\n",
    "    selected_action_contexts = action_context[bandit_feedback[\"action\"], :]\n",
    "    bandit_feedback_df = pl.DataFrame(\n",
    "        {\n",
    "            \"time_step\": [i for i in range(n_rounds)],\n",
    "            \"context\": bandit_feedback[\"context\"].tolist(),\n",
    "            \"action\": bandit_feedback[\"action\"].tolist(),\n",
    "            \"action_context\": selected_action_contexts.tolist(),\n",
    "            \"reward\": bandit_feedback[\"reward\"].tolist(),\n",
    "            \"p_score\": bandit_feedback[\"pscore\"].tolist(),\n",
    "        }\n",
    "    )\n",
    "    print(bandit_feedback_df)\n",
    "\n",
    "    # 評価方策を使って、ログデータ(bandit_feedback)に対する行動選択確率を計算\n",
    "    # target_policy_action_dist = target_policy_1(\n",
    "    #     context=bandit_feedback[\"context\"],\n",
    "    #     action_context=bandit_feedback[\"action_context\"],\n",
    "    #     recommend_arm_idx=0,\n",
    "    # )\n",
    "    target_policy_action_dist = target_policy_function(\n",
    "        context=bandit_feedback[\"context\"],\n",
    "        action_context=bandit_feedback[\"action_context\"],\n",
    "    )\n",
    "    # 真の期待報酬E_{p(r|x,a)}[r]を使って、データ収集方策の代わりに評価方策を動かした場合の性能を計算\n",
    "    ground_truth_policy_value = dataset.calc_ground_truth_policy_value(\n",
    "        expected_reward=bandit_feedback[\"expected_reward\"],\n",
    "        action_dist=target_policy_action_dist,\n",
    "    )\n",
    "\n",
    "    # OPE推定量を準備(naive推定量とIPS推定量)\n",
    "    naive_estimator = ReplayMethod()\n",
    "    ips_estimator = InverseProbabilityWeighting()\n",
    "\n",
    "    # それぞれのOPE推定量を使って、データ収集方策の代わりに評価方策を動かした場合の価値を推定\n",
    "    estimated_policy_value_by_naive = naive_estimator.estimate_policy_value(\n",
    "        reward=bandit_feedback[\"reward\"],\n",
    "        action=bandit_feedback[\"action\"],\n",
    "        action_dist=target_policy_action_dist,\n",
    "    )\n",
    "    estimated_policy_value_by_ips = ips_estimator.estimate_policy_value(\n",
    "        action=bandit_feedback[\"action\"],\n",
    "        reward=bandit_feedback[\"reward\"],\n",
    "        action_dist=target_policy_action_dist,\n",
    "        pscore=bandit_feedback[\"pscore\"],\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"simulate_idx\": simulate_idx,\n",
    "        \"ground_truth_policy_value\": ground_truth_policy_value,\n",
    "        \"estimated_policy_value_by_naive\": estimated_policy_value_by_naive,\n",
    "        \"estimated_policy_value_by_ips\": estimated_policy_value_by_ips,\n",
    "    }\n",
    "\n",
    "\n",
    "# シミュレーションの設定\n",
    "simulate_idx = 0\n",
    "n_rounds = 10000\n",
    "n_actions = 4\n",
    "dim_context = 3\n",
    "reward_type = \"binary\"\n",
    "action_context = np.random.random((n_actions, dim_context))\n",
    "reward_function = expected_reward_function\n",
    "logging_policy_function = policy_wrapper(pi_3, mode=\"logging_policy\")\n",
    "target_policy_function = policy_wrapper(pi_4, mode=\"target_policy\")\n",
    "\n",
    "# シミュレーションを実行\n",
    "result = run_single_simulation(\n",
    "    simulate_idx=simulate_idx,\n",
    "    n_rounds=n_rounds,\n",
    "    n_actions=n_actions,\n",
    "    dim_context=dim_context,\n",
    "    reward_type=reward_type,\n",
    "    action_context=action_context,\n",
    "    reward_function=reward_function,\n",
    "    logging_policy_function=logging_policy_function,\n",
    "    target_policy_function=target_policy_function,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-experiments-_FVBVT8O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
