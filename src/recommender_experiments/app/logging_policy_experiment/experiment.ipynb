{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masato.morita/Library/Caches/pypoetry/virtualenvs/recommender-experiments-_FVBVT8O-py3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from typing import Callable, Optional, TypedDict\n",
    "import numpy as np\n",
    "from obp.dataset import SyntheticBanditDataset\n",
    "from obp.ope import ReplayMethod, InverseProbabilityWeighting\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditFeedbackDict(TypedDict):\n",
    "    n_rounds: int  # ラウンド数\n",
    "    n_actions: int  # アクション数\n",
    "    context: np.ndarray  # 文脈 (shape: (n_rounds, dim_context))\n",
    "    action_context: (\n",
    "        np.ndarray\n",
    "    )  # アクション特徴量 (shape: (n_actions, dim_action_features))\n",
    "    action: np.ndarray  # 実際に選択されたアクション (shape: (n_rounds,))\n",
    "    position: Optional[np.ndarray]  # ポジション (shape: (n_rounds,) or None)\n",
    "    reward: np.ndarray  # 報酬 (shape: (n_rounds,))\n",
    "    expected_reward: np.ndarray  # 期待報酬 (shape: (n_rounds, n_actions))\n",
    "    pi_b: np.ndarray  # データ収集方策 P(a|x) (shape: (n_rounds, n_actions))\n",
    "    pscore: np.ndarray  # 傾向スコア (shape: (n_rounds,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4 0.3 0.2 0.2]\n",
      " [0.4 0.3 0.2 0.2]\n",
      " [0.4 0.3 0.2 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# 真の期待報酬関数 E_{p(r|x,a)}[r] を定義する\n",
    "def expected_reward_function(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"(アクションa, 文脈x)の各組み合わせに対する期待報酬 E_{p(r|x,a)}[r] を定義する関数\n",
    "    今回の場合は、推薦候補4つの記事を送った場合の報酬rの期待値を、文脈xに依存しない固定値として設定する\n",
    "    ニュース0: 0.2, ニュース1: 0.15, ニュース2: 0.1, ニュース3: 0.05\n",
    "    返り値のshape: (n_rounds, n_actions, len_list)\n",
    "    \"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 固定の期待報酬を設定 (n_actions=4として固定値を設定)\n",
    "    fixed_rewards = np.array([0.4, 0.3, 0.2, 0.2])\n",
    "\n",
    "    # 文脈の数だけ期待報酬を繰り返して返す\n",
    "    return np.tile(fixed_rewards, (n_rounds, 1))\n",
    "\n",
    "\n",
    "# 試しに期待報酬関数を実行してみる\n",
    "n_rounds = 3\n",
    "n_actions = 4\n",
    "context = np.array([[1], [2], [3]])\n",
    "action_context = np.array([[1], [2], [3], [4]])\n",
    "print(expected_reward_function(context, action_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1 0.1 0.1 0.7]\n",
      " [0.1 0.1 0.1 0.7]\n",
      " [0.1 0.1 0.1 0.7]]\n"
     ]
    }
   ],
   "source": [
    "# 5種類の推薦方策を定義していく\n",
    "\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "def pi_1(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([1.0, 0.0, 0.0, 0.0])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_2(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_1$ を確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([0.0, 1.0, 0.0, 0.0])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_3(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"コンテキストを考慮せず、全てのユーザに対してニュース $a_0$ を確率0.4、\n",
    "    ニュース $a_1$ を確率0.3、ニュース $a_2$ を確率0.2、ニュース $a_3$ を確率0.1で\n",
    "    推薦する確率的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    p_scores = np.array([0.4, 0.3, 0.2, 0.1])\n",
    "    action_dist = np.tile(p_scores, (n_rounds, 1))\n",
    "\n",
    "    assert action_dist.shape == (n_rounds, n_actions)\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_4(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"ユーザとニュースのコンテキストを考慮し、\n",
    "    コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを\n",
    "    確率1で推薦する決定的方策\"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 内積を計算\n",
    "    scores = context @ action_context.T  # shape: (n_rounds, n_actions)\n",
    "\n",
    "    # 各ラウンドで最もスコアが高いアクションのindexを取得\n",
    "    selected_actions = np.argmax(scores, axis=1)  # shape: (n_rounds,)\n",
    "\n",
    "    # 決定的方策: 確率1で最もスコアが高いアクションを選択\n",
    "    action_dist = np.zeros((n_rounds, n_actions))\n",
    "    action_dist[np.arange(n_rounds), selected_actions] = 1.0\n",
    "\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "def pi_5(\n",
    "    context: np.ndarray,\n",
    "    action_context: np.ndarray,\n",
    "    random_state: int = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"ユーザとニュースのコンテキストを考慮し、\n",
    "    コンテキストベクトル $x$ とアイテムコンテキストベクトル $e$ の内積が最も大きいニュースを\n",
    "    確率0.7で推薦し、その他のニュースを均等に確率0.1で推薦する確率的方策。\n",
    "    \"\"\"\n",
    "    n_rounds = context.shape[0]\n",
    "    n_actions = action_context.shape[0]\n",
    "\n",
    "    # 内積を計算\n",
    "    scores = context @ action_context.T  # shape: (n_rounds, n_actions)\n",
    "\n",
    "    # 各ラウンドで最もスコアが高いアクションのindexを取得\n",
    "    selected_actions = np.argmax(scores, axis=1)  # shape: (n_rounds,)\n",
    "\n",
    "    # 確率的方策: 確率0.4で全てのアクションを一様ランダムに選択し、確率0.6で最もスコアが高いアクションを決定的に選択\n",
    "    action_dist = np.full((n_rounds, n_actions), 0.1)\n",
    "    action_dist[np.arange(n_rounds), selected_actions] = 0.7\n",
    "    return action_dist\n",
    "\n",
    "\n",
    "print(\n",
    "    pi_5(\n",
    "        context=np.array([[1], [2], [3]]),\n",
    "        action_context=np.array([[1], [2], [3], [4]]),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]\n",
      "\n",
      " [[0.1]\n",
      "  [0.1]\n",
      "  [0.1]\n",
      "  [0.7]]]\n",
      "(3, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "def policy_wrapper(\n",
    "    base_policy: Callable[[np.ndarray, np.ndarray, int], np.ndarray],\n",
    "    mode: Literal[\"logging_poicy\", \"target_policy\"],\n",
    ") -> Callable[[np.ndarray, np.ndarray, int], np.ndarray]:\n",
    "    \"\"\"\n",
    "    方策のラッパー関数。データ収集時と評価時で出力の形状を切り替える。\n",
    "    (logging_policyの場合はshape=(n_rounds, n_actions)、target_policyの場合はshape=(n_rounds,n_actions, 1))\n",
    "    Parameters\n",
    "    - base_policy: ベースとなる方策関数\n",
    "    - mode: 方策のモード。\"logging_policy\"はデータ収集方策、\"target_policy\"は評価方策\n",
    "\n",
    "    Returns\n",
    "    - wrapされた方策関数\n",
    "    \"\"\"\n",
    "\n",
    "    def wrapped_policy(\n",
    "        context: np.ndarray,\n",
    "        action_context: np.ndarray,\n",
    "        random_state: int = None,\n",
    "    ) -> np.ndarray:\n",
    "        action_dist = base_policy(context, action_context, random_state)\n",
    "        if mode == \"logging_policy\":\n",
    "            return action_dist\n",
    "        elif mode == \"target_policy\":\n",
    "            return action_dist[:, :, np.newaxis]\n",
    "\n",
    "    return wrapped_policy\n",
    "\n",
    "\n",
    "target_policy = policy_wrapper(pi_5, mode=\"target_policy\")\n",
    "print(target_policy(context, action_context))\n",
    "print(target_policy(context, action_context).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender-experiments-_FVBVT8O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
